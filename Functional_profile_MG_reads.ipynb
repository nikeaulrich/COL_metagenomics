{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a1db36-72e0-41b8-b14b-374bcde9d2d9",
   "metadata": {},
   "source": [
    "## Functional annotation of reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c443fc-74db-49e6-bf62-b72b510347c5",
   "metadata": {},
   "source": [
    "Using HUMAnN 3.0: (the HMP Unified Metabolic Analysis Network) method for efficiently and accurately profiling the abundance of microbial metabolic pathways and other molecular functions from metagenomic or metatranscriptomic sequencing data. It is appropriate for any type of microbial community\n",
    "\n",
    "https://github.com/biobakery/biobakery/wiki/humann3#22-running-humann-the-basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561e8d9-7712-4f9f-bf92-4a572adf0793",
   "metadata": {},
   "source": [
    "resource: https://huttenhower.sph.harvard.edu/humann/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce3d84-7cf8-44ca-a375-af7b18c5a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLATION\n",
    "conda create --prefix /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3 python=3.7\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "#set conda channel priority\n",
    "conda config --add channels defaults\n",
    "conda config --add channels bioconda\n",
    "conda config --add channels conda-forge\n",
    "conda config --add channels biobakery\n",
    "conda install humann -c biobakery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a117b-e425-4b13-bf22-cee4d7bc9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download databases to do demo run\n",
    "humann_databases --download chocophlan DEMO humann_dbs\n",
    "humann_databases --download uniref DEMO_diamond humann_dbs\n",
    "#downloaded demo_seq file from: https://github.com/biobakery/biobakery/wiki/humann3\n",
    "#run test with demo data to make sure it works\n",
    "humann -i demo.fastq.gz -o sample_results (run this in a bash script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e701ce-df02-4b16-94b9-d94ebb98ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update databases\n",
    "humann_databases --download utility_mapping full /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs --update-config yes\n",
    "humann_databases --download chocophlan full /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs --update-config yes\n",
    "humann_databases --download uniref uniref90_diamond /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs --update-config yes\n",
    "humann_databases --download uniref uniref50_diamond /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs --update-config yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630c880-89d1-4685-82b9-705f464a4cfb",
   "metadata": {},
   "source": [
    "https://github.com/biobakery/humann?tab=readme-ov-file#main-workflow\n",
    "\n",
    "https://github.com/biobakery/biobakery/wiki/humann3#22-running-humann-the-basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116f366-7ee7-4005-82cb-1de203eb8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/humann_prof/pstr/slurm-humann-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "\n",
    "# Set parameters\n",
    "#SAMPLENAME=\"pstr\"\n",
    "#SAMPLELIST=\"032024_pstr_sampleids.txt\" \n",
    "#LISTPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/\"\n",
    "DB=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs\"\n",
    "READS=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/assembly/pstr/repaired\"\n",
    "INPUT=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/pstr\"\n",
    "#mkdir -p $INPUT\n",
    "OUT=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/humann_prof/pstr\"\n",
    "#mkdir -p $OUT\n",
    "\n",
    "#concatenate F and R\n",
    "#paired-end currently not taken into account during HUMAnN's alignment steps. The best way to use paired-end sequencing data with HUMAnN is simply to concatenate all reads into a single FASTA or FASTQ file. \n",
    "#Will concatenate the F and R reads for one sample to try.  \n",
    "cat $READS/032024_COL_SAN_T5_144_PSTR_S9_host_removed_R1.tagged_filter_ready.fastq.gz $READS/032024_COL_SAN_T5_144_PSTR_S9_host_removed_R2.tagged_filter_ready.fastq.gz > $INPUT/032024_COL_SAN_T5_144_PSTR_S9_filter_ready_all.fastq.gz\n",
    "\n",
    "#run on quality filtered and repaired reads that are concatenated\n",
    "humann --input $INPUT/032024_COL_SAN_T5_144_PSTR_S9_filter_ready_all.fastq.gz --output $OUT/ \n",
    "\n",
    "conda deactivate\n",
    "\n",
    "# JOB-ID: 29032744\n",
    "# bash script file name: nikea/COL/bash_scripts/Col_humann_profiling.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19663d10-c60a-4964-8902-0ad275e1ea14",
   "metadata": {},
   "source": [
    "notes: \\\n",
    "From humann manual: The best way to use paired-end sequencing data with HUMAnN is simply to concatenate all reads into a single FASTA or FASTQ file. Will use the concatenated seqs for each coral species first. \\\n",
    "However, another option could be to interleave the reads. resource: https://www.biostars.org/p/9469504/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a9105-fc1a-41af-af7a-01c5ade5a1b3",
   "metadata": {},
   "source": [
    "Ran basch script from /scratch folder but had humann running in work. Temp files are large so will change to run everything in /scratch.\n",
    "\n",
    "ran on one sample first to test, but might need to run on all samples concatenated together depending on the output (so far looks like metaphlan was unable to identify taxa in this one sample)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a08869d9-8a27-4946-a7d4-2ac2f7035dec",
   "metadata": {},
   "source": [
    "output log:\n",
    "\n",
    "Total species selected from prescreen: 0\n",
    "\n",
    "Selected species explain 0.00% of predicted community composition\n",
    "\n",
    "No species were selected from the prescreen.\n",
    "Because of this the custom ChocoPhlAn database is empty.\n",
    "This will result in zero species-specific gene families and pathways.\n",
    "\n",
    "Running diamond ........\n",
    "\n",
    "Aligning to reference database: uniref90_demo_prots_v201901b.dmnd\n",
    "\n",
    "Aligning to reference database: uniref90_201901b_full.dmnd\n",
    "\n",
    "Total bugs after translated alignment: 1\n",
    "unclassified: 373109 hits\n",
    "\n",
    "Total gene families after translated alignment: 4124\n",
    "\n",
    "Unaligned reads after translated alignment: 49.5879702905 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6485b10-6468-4387-93fe-9e29091a2e8b",
   "metadata": {},
   "source": [
    "**Let's try all ofav samples and see if that helps!** this is following a meeting where it was decided that we should focus on ofav first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bf843-fb1d-4340-b0f1-037ea6a3bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/ofav/take2/slurm-humann-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "\n",
    "# Set parameters\n",
    "#PATH=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/ofav\"\n",
    "\n",
    "#concatenate F and R\n",
    "#paired-end currently not taken into account during HUMAnN's alignment steps. The best way to use paired-end sequencing data with HUMAnN is simply to concatenate all reads into a single FASTA or FASTQ file. \n",
    "#Will concatenate the F and R reads of all samples.\n",
    "#cat $READS/ofav_reads_R1_ALL.fastq.gz $READS/ofav_reads_R2_ALL.fastq.gz > $PATH/ofav_reads_ALL.fastq.gz\n",
    "\n",
    "#run humann\n",
    "humann --input humann_prof/ofav/ofav_reads_ALL.fastq.gz --output humann_prof/ofav/take2 --threads 11\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "# JOB-ID: 29185909, take 2(29227386)\n",
    "# bash script file name: nikea/COL/bash_scripts/Col_humann_profiling_ofav.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ce140-a679-4b74-81da-79c8eb8eaf87",
   "metadata": {},
   "source": [
    "**downloaded the uniref50 database and re-run, hopefully this will help annotate the gene families!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3d98d-6844-404f-b370-da006e038de0",
   "metadata": {},
   "source": [
    "looking at data, using this resource: https://github.com/biobakery/biobakery/wiki/humann3 \\\n",
    "https://github.com/biobakery/humann?tab=readme-ov-file#humann_barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4eb94c-8557-4c3e-bd9c-a456329f9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "salloc -p cpu --mem 150G\n",
    "#salloc: Granted job allocation 29241322\n",
    "#salloc: Nodes uri-cpu001 are ready for job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2dd137-90a3-49e2-981f-a0831c8879a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "#lets normalize to copies per million\n",
    "humann_renorm_table --input humann_prof/ofav/take2/ofav_reads_ALL_genefamilies.tsv --output humann_prof/ofav/take2/ofav_reads_ALL_genefamilies_cpm.tsv --units cpm --update-snames\n",
    "cd humann_prof/ofav/take2/\n",
    "humann_regroup_table --input ofav_reads_ALL_genefamilies_cpm.tsv --output rxn-cpm.tsv --groups uniref90_rxn\n",
    "#Original Feature Count: 13153; Grouped 1+ times: 465 (3.5%); Grouped 2+ times: 138 (1.0%)\n",
    "#now lets attach some human-readable descriptions of these IDs to facilitate biological interpretation\n",
    "humann_rename_table --input rxn-cpm.tsv --output rxn-cpm-named.tsv --names metacyc-rxn\n",
    "#Renamed 554 of 556 entries (99.64%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491714c-2b83-4758-bf2e-028b1e406e29",
   "metadata": {},
   "source": [
    "let's try using the assembled contigs rather than the filtered fastq.gz to see if that yields better taxonomic results! **It doesn't so don't do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6f9eb-ecfd-4526-a268-23c6131bc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/ofav/ofav_contigs/slurm-humann-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "\n",
    "# Set parameters\n",
    "CONTIGSPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/assembly/ofav/megahit_assembly\"\n",
    "\n",
    "#trying the ofav assembled contigs to see what difference it makes\n",
    "\n",
    "#run humann\n",
    "humann --input $CONTIGSPATH/ofav.contigs.fa --output humann_prof/ofav/ofav_contigs --threads 11\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "# JOB-ID: 29250150\n",
    "# bash script file name: nikea/COL/bash_scripts/Col_humann_profiling_ofav.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055ee2a-979c-437a-bf18-7065698a043e",
   "metadata": {},
   "source": [
    "Concatenating each separate sample ID and running humann. Then will combine files at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e245a-98ce-4cfc-a986-d81be3e6ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 48:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/ofav/slurm-humann-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "\n",
    "# Set parameters\n",
    "SAMPLENAME=\"ofav\"\n",
    "SAMPLELIST=\"032024_${SAMPLENAME}_sampleids.txt\" \n",
    "LISTPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/\"\n",
    "#DB=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_dbs\"\n",
    "READS=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/assembly/${SAMPLENAME}/repaired\"\n",
    "\n",
    "#work from scratch3 directory (submit script from there)\n",
    "\n",
    "#concatenate F and R\n",
    "while IFS= read -r SAMPLEID; do\n",
    "cat $READS/\"${SAMPLEID}\"_host_removed_R1.tagged_filter_ready.fastq.gz $READS/\"${SAMPLEID}\"_host_removed_R2.tagged_filter_ready.fastq.gz > humann_prof/ofav/\"${SAMPLEID}\"_filter_ready_all.fastq.gz\n",
    " if [ $? -eq 0 ]; then\n",
    "        echo \"concatenation successful for sample: $SAMPLEID\"\n",
    "    else\n",
    "        echo \"encountered an error for sample: $SAMPLEID\"\n",
    "        exit 1\n",
    "    fi\n",
    "done < \"$LISTPATH/${SAMPLELIST}\"\n",
    "\n",
    "#run humann on quality filtered and repaired reads that are concatenated\n",
    "#run in scratch because the temp files are very big\n",
    "cd humann_prof/\"${SAMPLENAME}\"\n",
    "\n",
    "while IFS= read -r SAMPLEID; do\n",
    "humann --input \"${SAMPLEID}\"_filter_ready_all.fastq.gz --output \"${SAMPLEID}\"_humann --threads 11\n",
    " if [ $? -eq 0 ]; then\n",
    "        echo \"humann profile completed for sample: $SAMPLEID\"\n",
    "    else\n",
    "        echo \"humann encountered an error for sample: $SAMPLEID\"\n",
    "        exit 1\n",
    "    fi \n",
    "done < \"$LISTPATH/${SAMPLELIST}\"\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "# JOB-ID: 29436670 (forgot to add more threads so first sample ran before I cancelled and re-ran)\n",
    "# bash script file name: nikea/COL/bash_scripts/Col_humann_indiv.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72161d60-c77f-4a72-86f6-761eae9418db",
   "metadata": {},
   "source": [
    "metaphlan was unable to classify taxa in each sample. For this I either should concatenate all samples or go the kraken route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b5ad7-a89c-4187-877e-2ee93371abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the table, combine samples into one table!\n",
    "#move output tables from /scratch to /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/humann_prof/ofav\n",
    "\n",
    "humann_join_tables --input ./ --output combined_tables_genefamilies.tsv\n",
    "\n",
    "humann_renorm_table --input combined_tables_genefamilies.tsv --output combined_tables_genefamilies_cpm.tsv --units cpm --update-snames\n",
    "\n",
    "humann_regroup_table --input combined_tables_genefamilies_cpm.tsv --output rxn-combined_cpm.tsv --groups uniref50_rxn\n",
    "\n",
    "humann_rename_table --input rxn-combined_cpm.tsv --output rxn-combined_cpm_named.tsv --names metacyc-rxn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa53800-f824-4b65-876e-62746956da58",
   "metadata": {},
   "source": [
    "I think the next step is to concatenate all samples for each (mcav, dlab, pstr) and then compare across those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d100a1-3691-48a0-8ad5-6199d944d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/mcav/slurm-humann-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/biobakery3\n",
    "\n",
    "# Set parameters\n",
    "READS=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/assembly/mcav\"\n",
    "NEWPATH=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/humann_prof/mcav\"\n",
    "mkdir -p $NEWPATH\n",
    "\n",
    "#concatenate F and R\n",
    "#paired-end currently not taken into account during HUMAnN's alignment steps. The best way to use paired-end sequencing data with HUMAnN is simply to concatenate all reads into a single FASTA or FASTQ file. \n",
    "#Will concatenate the F and R reads of all samples.\n",
    "cat $READS/mcav_reads_R1_ALL.fastq.gz $READS/mcav_reads_R2_ALL.fastq.gz > $NEWPATH/mcav_reads_ALL.fastq.gz\n",
    "\n",
    "#run humann\n",
    "humann --input humann_prof/mcav/mcav_reads_ALL.fastq.gz --output humann_prof/mcav --threads 11\n",
    "\n",
    "conda deactivate\n",
    "\n",
    "# JOB-ID: 29464798\n",
    "# bash script file name: nikea/COL/bash_scripts/Col_humann_profiling_mcav.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a519fab-b495-4713-94c8-f980abfbe30c",
   "metadata": {},
   "source": [
    "ran this for dlab (29465661; needed 32 hrs to run) and pstr (29465821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e30d9d-8d58-4273-9ba9-34ea60ac4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined all species into one table genefamilies file (moved to same directory)\n",
    "humann_join_tables -i ./ -o all_species_genefamilies.tsv\n",
    "\n",
    "humann_renorm_table --input all_species_genefamilies.tsv --output all_species_genefamilies_cpm.tsv --units cpm --update-snames \n",
    "\n",
    "humann_regroup_table --input all_species_genefamilies_cpm.tsv --output all_species_rxn_cpm.tsv --groups uniref90_rxn\n",
    "\n",
    "humann_rename_table --input all_species_rxn_cpm.tsv --output all_species_rxn_cpm_named.tsv --names metacyc-rxn \n",
    "\n",
    "#combine all species into one pathpathabundances file using join_tables command\n",
    "\n",
    "#combine metaphlan bugs list for all 4 coral species\n",
    "#https://github.com/biobakery/biobakery/wiki/metaphlan4\n",
    "merge_metaphlan_tables.py dlab_reads_ALL_metaphlan_bugs_list.tsv mcav_reads_ALL_metaphlan_bugs_list.tsv ofav_reads_ALL_metaphlan_bugs_list.tsv pstr_reads_ALL_metaphlan_bugs_list.tsv > merged_metaphlan_abundance_table.txt\n",
    "\n",
    "conda install -c biobakery hclust2\n",
    "\n",
    "#for some reason this removes the headers to made sure to add them back in and re-upload\n",
    "#this creates a species only abundance table (can adapt this for other taxonomic levels too)\n",
    "grep -E \"s__\" merged_metaphlan_abundance_table.txt \\\n",
    "| grep -v \"t__\" \\\n",
    "| sed \"s/^.*|//g\" \\\n",
    "> merged_species_abundance_table_species.txt\n",
    "\n",
    "#generate the heatmap, doesn't work yet\n",
    "\n",
    "hclust2.py \\\n",
    "-i merged_species_abundance_table_species.txt \\\n",
    "-o metaphlan_abundance_heatmap_species.png \\\n",
    "--f_dist_f braycurtis \\\n",
    "--s_dist_f braycurtis \\\n",
    "--cell_aspect_ratio 0.5 \\\n",
    "--flabel_size 10 --slabel_size 10 \\\n",
    "--max_flabel_len 100 --max_slabel_len 100 \\\n",
    "--minv 0.1 \\\n",
    "--minv 0 --maxv 100\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
